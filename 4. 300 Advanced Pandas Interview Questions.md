<!--- Thanks! I’ll now compile 300 advanced-level, interview-focused questions covering all major aspects of Pandas and data manipulation, with a primary emphasis on Pandas. This will include indexing, reshaping, group operations, joins, performance optimization, datetime handling, and more.

I’ll get back to you shortly with a detailed list of expertly curated questions. -->


# 300 Advanced Pandas Interview Questions

**Preparation Note:** The questions below are categorized by topic for clarity. Each section focuses on a key area of Pandas and related data manipulation topics. These are advanced-level questions aimed at experienced data professionals. Use them to assess deep understanding, tricky nuances, and real-world scenarios in Pandas usage.

## DataFrame Creation, Indexing, and Slicing

1. How do the `.loc` and `.iloc` indexers differ when selecting subsets of data from a DataFrame? Provide an example of each.
2. What are the `.at` and `.iat` methods used for in pandas, and how do they differ from `.loc` and `.iloc` in terms of functionality and performance?
3. How can you set or change the index of a DataFrame? What is the difference between using `set_index` (or assigning to `df.index`) and using `reset_index`?
4. What is the effect of using a single set of brackets vs. double brackets when selecting columns from a DataFrame (e.g., `df['col']` vs `df[['col']]`)?
5. How can you randomly sample a subset of rows from a DataFrame? For example, how would you select 5% of the rows at random?
6. How does the pandas `.query()` method work and what are its advantages over traditional boolean indexing? Give an example where `.query` might make a filtering operation more readable.
7. What is the `pd.eval()` function and how can it be used to optimize operations on pandas objects (for example, performing a complex expression across columns)?
8. When adding two Series or DataFrames with non-aligned indices, how does pandas handle the operation? Explain the concept of **data alignment** with an example.
9. When would you use `DataFrame.reindex()`? How is reindexing different from filtering a DataFrame with `.loc`?
10. How does slicing a DataFrame with `.loc` differ from slicing with `.iloc`, especially regarding the treatment of the end index?
11. What is the correct way to filter a DataFrame on multiple conditions (e.g., rows where `colA > 5` **and** `colB < 10`)? Why can’t you use the `and` operator for this?
12. If a DataFrame’s index contains duplicate labels, how does that affect the behavior of selection with `.loc` for those labels?
13. How do you sort a DataFrame by its index? How would you sort it by its column names?
14. What does the `DataFrame.where()` method do? How is using `where()` to filter or replace data different from using direct boolean indexing?
15. How can you replace all negative values in a DataFrame with zero in one operation, without using a Python loop?
16. What is *chained indexing* in pandas and why can it lead to problems? Provide an example of chained indexing that might produce a warning.
17. What does a **SettingWithCopyWarning** indicate? How can you avoid this warning when you need to modify a subset of a DataFrame?
18. Many pandas methods have an `axis` parameter. For example, what is the difference between `df.sum(axis=0)` and `df.sum(axis=1)`?
19. How can you select DataFrame columns based on a pattern in their names (for example, all columns that start with a certain prefix or end with a certain suffix)?
20. What is the most efficient way to get or set a single value in a DataFrame given its row and column labels?
21. How can you filter a DataFrame to include only rows where a column’s value is *in* a given list of values (for example, where column `"City"` is either `"Paris"`, `"London"`, or `"Tokyo"`)?
22. In pandas, what is the difference between doing `df.loc[mask, 'col'] = value` versus `df['col'][mask] = value` when setting values? Which one is preferred and why?
23. What is the purpose of the `DataFrame.pipe()` method? How does it help in writing cleaner code or method chaining with pandas?
24. How do you rename columns or index labels in a DataFrame? Provide at least two different ways to achieve this.
25. How can you add a new column to a DataFrame that is a function of existing columns (for example, the sum or ratio of two other columns)?
26. What is the difference between `DataFrame.apply()` and `DataFrame.applymap()`? When would you use one over the other?
27. How can you convert the index of a DataFrame into a column, or vice versa? (Explain the use of `reset_index()` and `set_index()`.)
28. How can you reorder or rearrange the columns of a DataFrame? For example, how would you move the last column to become the first column?
29. Many DataFrame operations (like `drop`, `fillna`, etc.) have an `inplace` option. What does `inplace=True` do, and why do some experts recommend avoiding it in pandas code?
30. How can you combine or concatenate the values of two columns into a single column? (For instance, merging a first name and last name column into a full name string.)
31. If a DataFrame column contains lists of values, which pandas method would you use to transform each element of that list into separate rows (expanding the list so each item becomes its own row)?
32. Name a few ways to construct a pandas DataFrame from external data structures (like Python dictionaries, lists of tuples, another DataFrame, NumPy arrays, etc.).
33. How can you quickly check the number of rows and columns in a DataFrame and the total number of elements it contains?
34. How can you identify rows in a DataFrame where a certain column’s values are duplicated? Once identified, how might you select or isolate those rows?
35. When reading data from a CSV using `pd.read_csv`, how can you make one of the columns directly become the DataFrame’s index?
36. How do you concatenate two DataFrames vertically (stacking rows one below the other)? How do you concatenate them horizontally (adding columns side by side)?
37. What is the difference between using `df.append(other_df)` and `pd.concat([df, other_df])` to combine DataFrames? Why is using `df.append` inside a loop discouraged for large datasets?
38. After filtering a DataFrame, the index may no longer be sequential (since it retains the original row labels). How do you reset the index of the filtered DataFrame to clean it up (and optionally drop the old index)?
39. How does pandas ensure that operations like adding two DataFrames align on their indexes? What happens if one DataFrame lacks an index label that the other has?
40. How would you pivot a DataFrame from a *long* format to a *wide* format (creating new columns from unique values of one feature)? What method would you use, and what are the prerequisites for using it?

## GroupBy and Aggregation

1. What is the *split-apply-combine* strategy in data analysis, and how does the pandas `groupby` operation embody this paradigm?
2. How do you perform multiple aggregation functions in a single groupby operation? For example, how would you get both the mean and sum of a value column for each group in one go (in a single grouped result)?
3. In pandas groupby, what is the difference between the `agg` (or `aggregate`), `apply`, and `transform` methods? When would you use each of these in practice?
4. How can you add a new column to a DataFrame that gives a group-level summary for each row? (For example, adding a column with the **total sales per group** or the **mean value per group** next to each row of that group.)
5. What does `groupby().filter()` do, and how is it different from simply filtering a DataFrame before or after grouping? Provide a scenario where `groupby().filter` is useful.
6. After grouping data with pandas, how can you sort the aggregated results by the group key or by one of the computed summary values?
7. If you group by multiple columns, what is the structure of the resulting index in the aggregated DataFrame? How can you convert that MultiIndex to flat columns if desired?
8. By default, does pandas sort the group keys when using `groupby`? How can you preserve the original order of appearance of the groups instead?
9. How are **NaN** values treated in the grouping key when using `DataFrame.groupby`? For instance, if your DataFrame has missing values in the group-by column, do those rows appear in the grouped result? How can you include them if needed?
10. When grouping by a datetime field to resample data (say, converting daily data to monthly), what pandas tools or parameters can you use? *(Hint: consider using `pd.Grouper` with a frequency, or the `.resample` method if the data is time-indexed.)*
11. How would you compute a rolling or moving aggregation *within each group* of a DataFrame? (For example, a 3-day moving average for each city in a dataset of daily city temperatures.)
12. Explain how you would calculate the percentage contribution of each group to the total. (For example, each group’s sum divided by the overall sum, yielding a percentage for each group.)
13. What is the purpose of the `as_index` parameter in `DataFrame.groupby`? How does the output differ if `as_index=False` is used when grouping a DataFrame?
14. How can you group by a *transformation* or *derived value* of a column without first adding a new column? (For example, grouping by the year of a datetime column, or grouping numeric ages into bins.)
15. Give an example of using `groupby` on a DataFrame that has a MultiIndex (hierarchical index). How do you specify the level(s) you want to group by?
16. In a grouped aggregation, what is the difference between using `.size()` and `.count()`? When might they yield different results?
17. How can you apply a custom aggregation function that isn’t built-in (for example, a harmonic mean or a custom statistical measure) over each group in a DataFrame?
18. Imagine you have a DataFrame of sales transactions with columns `[Region, Product, Sales]`. How would you compute the **top 3 Products by total Sales within each Region** using pandas?
19. Pivot tables can be used as an alternative to groupby for summarizing data. What capabilities does `pivot_table` have that basic groupby aggregations lack?
20. Sometimes after grouping and aggregating, you want to combine the result back with the original DataFrame (for example, to tag each row with some group statistic). How can you merge or join the groupby result back onto the original DataFrame efficiently?
21. How would you use groupby to **normalize data within groups** (e.g., subtract the mean of each group from the values in that group)?
22. Can you group by a mapping or dictionary of values rather than by a column directly? (Hint: yes, pandas allows grouping by a Series or dict that maps index labels to group labels.)
23. What exactly is a “GroupBy object” that pandas returns when you call `df.groupby(...)`? How is it different from the original DataFrame, and what kinds of operations can you perform on it?
24. If you have done `df.groupby('Category')['Value'].mean()`, how can you convert the resulting Series into a DataFrame with the category as a normal column (not index) and the mean as another column?
25. Explain or give an example where using `groupby().apply()` is necessary or more convenient than `groupby().agg()`. (For instance, when the operation returns a DataFrame of a different shape for each group.)
26. How would you calculate a **cumulative sum within each group**? (For example, cumulative sales per customer over time, resetting at each new customer.)
27. How do you group data by *ranges or bins* of values? For instance, grouping people by age range (0–9, 10–19, 20–29, etc.) and counting how many fall into each bin.
28. GroupBy and transformation: If you need to compute an aggregation and broadcast it back to the original DataFrame (producing an output of the same shape as the input), which groupby method would you use? *(Hint: `.transform`.)*
29. Is it possible to perform a groupby aggregation *across columns* instead of across rows? (Hint: yes, you can group by the axis; for example, `df.groupby(axis=1)`, although it’s not common.)
30. How can you use the `key` parameter in sorting or grouping to group by a computed key on the fly (without adding a new column)? *(For example, grouping by the lowercase version of a column, or grouping by a substring of a string key.)*

## Merging, Joining, and Concatenation

1. What is the difference between `pd.merge` and `pd.concat` when combining DataFrames? In what situations would you use one over the other?
2. Compare the use of `DataFrame.join()` vs `pd.merge()`. How do these methods differ in how you specify keys and in their default join behavior?
3. Explain the various types of joins in pandas (`inner`, `left`, `right`, `outer`). What is the default join type for `pd.merge` and for `DataFrame.join`?
4. How would you merge two DataFrames on multiple columns (i.e. a composite key)? Provide a brief example or explanation.
5. How can you merge two DataFrames based on their *index* values rather than a column? (Describe the parameters or approach you would use with `merge` or `join`.)
6. If you perform an **outer join** on two DataFrames that don’t have matches for some keys on one side, how are the results represented in the output? (Hint: what will you see for columns from the “missing” side?)
7. When merging DataFrames, you might get overlapping column names in the result. How can you control the suffixes for overlapping names in `pd.merge` to avoid confusion?
8. What does setting `indicator=True` do in a pandas merge? How can the information from the `_merge` column be used in data analysis (give an example)?
9. How can you perform a Cartesian join (cross join) of two tables with pandas? Provide an example of using `pd.merge` (or another method) to do a cross join.
10. Suppose you have one DataFrame with daily data and another with monthly data, and you want to merge them by matching each day to the corresponding month period. How could you accomplish this merge? *(Hint: consider merging on year/month or using an interval join if available.)*
11. What happens if you merge two DataFrames and both have duplicate values in the join key (i.e. a many-to-many merge)? How will the number of rows in the result relate to the input row counts?
12. When concatenating multiple DataFrames vertically, how can you reset the index or ignore the original indices? Illustrate how the `ignore_index` parameter in `pd.concat` works.
13. You have a list of DataFrames `dfs` that all have the same columns. How do you concatenate them into one DataFrame? What is a potential pitfall of using `df1.append(df2)` repeatedly in a loop to achieve this?
14. How can you concatenate DataFrames **horizontally** (side by side) and what must be true about those DataFrames for this to work correctly?
15. What is the purpose of the `keys` argument in `pd.concat`? How does it affect the resulting DataFrame’s index when concatenating multiple DataFrames?
16. How would you combine two DataFrames that have different sets of columns (like a SQL full outer union of columns)? (Hint: Use `pd.concat` with `axis=0`; missing columns will be filled with NaN.)
17. Explain the function of `verify_integrity` in `pd.concat`. When might you want to set `verify_integrity=True` and what happens if the integrity check fails?
18. If you needed to merge more than two DataFrames on the same key, how could you accomplish this in pandas?
19. What is `DataFrame.combine_first` used for? How is it different from a standard merge or concat operation?
20. How can you merge a DataFrame with another small lookup table (for example, mapping product IDs to product names) **without** using `pd.merge` explicitly? *(Hint: consider Series mapping with `.map()` or using `.replace()` with a dictionary.)*
21. After merging or concatenating, you sometimes end up with a **hierarchical index** (MultiIndex) either on rows or columns. What steps can you take to flatten the index back into a single level (restore normal index/columns)?
22. When performing a merge, what does `how='outer'` do? How is it different from `how='inner'` in terms of which rows appear in the result?
23. Can you merge on a condition other than equality (for example, merge where a timestamp from one table falls between start and end timestamps of another)? If pandas doesn’t support it directly, what approaches could you use to achieve this?
24. How do you join two DataFrames so that you keep **all rows from the left** DataFrame but only matching rows from the right DataFrame? What is this type of join called, and how do you specify it in code?
25. If you have two DataFrames with the same index, how can you merge or combine them while preserving the index (instead of turning it into a column)? *(Hint: consider using `pd.concat` with `axis=1` or the DataFrame `.join()` method.)*

## Handling Missing Data

1. How do you check for missing values in a pandas DataFrame or Series? What methods can you use to identify NaNs (give at least one)?
2. What is the difference between `isnull()` and `notnull()` (or their aliases `isna()` and `notna()`) in pandas?
3. How would you count the number of missing values in each column of a DataFrame?
4. Explain the different options available with the `dropna()` method. How would you drop (a) any row with a missing value, (b) rows where *all* values are missing, or (c) drop columns with missing values?
5. How can you drop rows that have missing values in *only certain columns* (while ignoring NaNs in other columns)?
6. What is the purpose of the `thresh` parameter in `dropna()`? Give an example of how it can be used.
7. Describe how you would fill missing values in a DataFrame using `fillna()`. What are some common strategies for choosing the fill value?
8. What are forward-fill and back-fill in the context of missing data? How do you perform a forward fill on a time-indexed DataFrame? *(Explain or demonstrate the use of `method='ffill'` or `'bfill'`.)*
9. How can you fill missing values with a computed value, such as the mean or median of that column?
10. Pandas provides an `interpolate()` function for filling in missing values. In what situations would interpolation be useful, and what are some interpolation methods supported?
11. If you have a time series with missing dates (gaps in the index), how can you **reindex** or use `resample` to introduce those missing periods, and then fill values for them appropriately?
12. What is the difference between `NaN` (NumPy’s not-a-number) and `None` in pandas? How does pandas treat these by default in numeric arrays versus object dtype arrays?
13. Pandas introduced a new `pd.NA` value for missing data. What advantages does `pd.NA` have over `np.nan`, and in what contexts would you see `pd.NA` used?
14. Why might an integer column become a float after you introduce a NaN into it? How can you maintain an integer type even with missing values present in that column?
15. How would you fill missing values in one column with values from another column (for example, use a “backup” column’s value whenever the primary column is NA)?
16. How can you fill missing values in a DataFrame using different fill values for different columns in one call (hint: passing a dict to `fillna`)?
17. Explain how to fill missing values with a **group-specific** value. For instance, how would you fill NaNs in a “Salary” column with the mean salary of each Department group?
18. Do pandas aggregation functions like `mean()`, `sum()`, etc. include or exclude NaN values by default? What about methods like `count()`?
19. How can you replace all NaN values in a DataFrame with a sentinel value (like -1 or `"Unknown"`)? Is there any difference between using `fillna()` and using `df.replace(np.nan, value)` for this?
20. What does the `combine_first` method do in the context of missing data? How could you use it to fill missing values of one DataFrame with values from another DataFrame?

## Time Series and DateTime Handling

1. How do you convert a column of strings or integers into actual datetime objects in pandas? For example, converting a string `"2025-12-31"` into a `datetime64` type in a DataFrame column.
2. What is a DateTimeIndex in pandas and how do you create one? Why might you want to use a DateTimeIndex for time series data?
3. How would you extract specific components from a datetime (like the year, month, day, hour, or day of week) for each entry in a datetime Series or index?
4. Explain the difference between `tz_localize` and `tz_convert` in pandas. When dealing with time zone–aware timestamps, when would you use each?
5. You have a DataFrame with a DateTimeIndex and you want to resample it to a lower frequency (e.g., convert daily data to monthly averages). Which pandas function or method would you use, and how does it work?
6. What are some common frequency strings (offset aliases) you can use with pandas date ranges and resampling? (For example: `'B'`, `'D'`, `'W'`, `'M'`, `'Q'`, `'A'`. Explain what a couple of these mean.)
7. How can you generate a sequence of dates in pandas, for example all days between January 1, 2023 and December 31, 2023? *(Hint: see `pd.date_range`.)*
8. If you have a DateTimeIndex, how can you select a slice of the DataFrame for a particular date or range of dates using a string? (For example, using `df['2023-05']` to get all data in May 2023.)
9. What is a pandas *Period* and how does it differ from a Timestamp? When might you use a PeriodIndex instead of a DateTimeIndex?
10. How do you compute the time difference between two datetime columns in pandas? What type of object or dtype is the result (and what units does it use by default)?
11. What is a pandas Timedelta? How can you create a Timedelta of 5 days or 10 hours, and how would you add it to a Timestamp?
12. How can you shift a time series by a certain number of periods or by a DateOffset? For example, how would you create a new column that is the data from 7 days ago for each date?
13. When working with financial time series or similar data, how might you get the last business day of each month or the first business day of each month? *(Hint: using date offsets like `MonthEnd` or `BM` for business month end.)*
14. How do you handle Daylight Savings Time transitions with pandas timestamps? For example, what happens if you localize naive timestamps to a DST-aware timezone like `'US/Eastern'`, and then convert to another timezone?
15. Explain how the `.resample()` method differs from `.groupby()` for time series data. In what situations would you use `resample` instead of `groupby` to summarize time-based data?
16. If you have an irregular time series (with gaps in the dates), how can you reindex or fill in the missing dates? What methods would you use to fill or interpolate values for those missing timestamps?
17. What does the `.dt` accessor provide in pandas? Give a few examples of properties or methods you can access via `Series.dt` for datetime Series.
18. How can you round or floor all timestamps in a Series to a certain frequency (e.g., round down to the start of the hour or start of the day for each timestamp)?
19. How would you create a new column indicating the time elapsed since the first record for each entry in a DataFrame? (For example, a “tenure” or “duration” measure using dates, perhaps via subtraction to get a Timedelta.)
20. Pandas can plot time series data easily. How do you ensure that the x-axis ticks are nicely formatted as dates when plotting a DataFrame that has a DateTimeIndex? *(This is often handled automatically by pandas/Matplotlib, but you might mention using `plt.gcf().autofmt_xdate()` or similar if needed.)*

## Window Functions and Moving Operations

1. What is a *rolling window* calculation in pandas? Give an example of computing a rolling mean on a Series.
2. How would you compute a 7-day moving average on a time series DataFrame? What pandas code would you use?
3. Explain the difference between a rolling window and an expanding window. In what scenario would you use an expanding window instead of a rolling window?
4. What does the `min_periods` parameter do in rolling operations? Provide an example of when you might want to use `min_periods`.
5. How can you apply a custom function to a rolling window in pandas? For example, if you wanted a rolling window to compute a custom statistic not provided by pandas, how would you do it?
6. Pandas rolling operations can be **centered**. What does setting `center=True` do in a rolling calculation?
7. What is an **exponentially weighted moving average** (EWMA)? How do you compute it using pandas, and how does it differ from a simple moving average?
8. In the context of `DataFrame.rolling().apply()`, what is the purpose of the `raw` parameter? What’s the difference between `raw=True` and `raw=False` for the function you apply?
9. How would you calculate a moving window over a time-based interval instead of a fixed number of observations? (Hint: using a time offset like `rolling('30D')` on a DateTimeIndex.)
10. If you call `df.rolling(window=5).mean()` on a DataFrame with multiple columns, how is the calculation applied to the DataFrame? (Explain how the rolling mean is computed for DataFrames vs Series.)
11. How can you compute the difference between consecutive rows in a Series or column? What method would you use, and what is the result for the first element?
12. How do you calculate the percent change between consecutive elements in a Series or between periods in a DataFrame (e.g., month-over-month percentage change)?
13. Suppose you want to apply a rolling window calculation and you have a custom function that could be sped up. What pandas feature allows you to specify an engine like `'numba'` to speed up a rolling apply?
14. What does the `expanding()` function do? How does an expanding mean differ from a rolling mean?
15. Provide an example of a cumulative operation in pandas and explain the output (for instance, `cumsum`, `cumprod`, or `cummax` on a Series).
16. How could you compute a rolling correlation between two columns in a DataFrame (say, a 30-day rolling correlation)?
17. What happens if you try to use a rolling operation on a DataFrame that has fewer rows than the window size? How does pandas handle the beginning of the series in that case?
18. Can you use rolling operations on higher-dimensional data like a Panel (3D) or a 3D NumPy array via pandas? (If not directly, what is the modern approach to applying rolling logic across multiple dimensions?)
19. Explain how you might detect peaks or local maxima in a time series using a rolling window technique.
20. How would you use a rolling window to smooth a noisy time series? (Conceptually describe the approach, e.g., using a rolling mean or rolling median to reduce noise.)

## Categorical and Text Data Manipulation

1. What is a pandas *Categorical* data type? Why and when would you use it instead of a regular object, int, or float dtype for a column?
2. How do you convert a regular column to a categorical type in pandas? Provide an example of creating a categorical Series.
3. What are the benefits of using categorical data in pandas? Mention at least two advantages (for example, memory usage and performance gains) of the `category` dtype.
4. How can you define an order for a categorical variable? For example, if you have a column “Size” with values *Small, Medium, Large*, how would you ensure they sort in a logical order (Small < Medium < Large) instead of alphabetically?
5. Once you have a categorical Series, how can you add a new category to it or remove categories that are not used in the data? (Hint: consider the `.cat` accessor methods.)
6. If you have a categorical Series and you try to assign a value that is not currently one of its categories, what happens? How can you allow new categories to be added dynamically?
7. How do you get the underlying integer *codes* of a categorical Series? How can those codes be useful?
8. Explain the output of `value_counts()` on a categorical Series versus a normal Series. Does it show categories with zero count?
9. What is the difference between storing data as `category` dtype vs. one-hot encoding it? When might you prefer to use a categorical dtype over expanding into dummy variables (and vice versa)?
10. How do you generate one-hot encoded dummy variables for a categorical column in pandas?
11. When reading a large CSV file, how might using categoricals help with memory usage? Describe what you would do to leverage categorical dtype for a high-cardinality text column.
12. In pandas, what is the `.str` accessor used for? Why can’t you directly use Python’s string methods on a Series without this accessor?
13. How would you convert all strings in a DataFrame column to lowercase using pandas (without writing an explicit loop)?
14. How can you check if each string in a Series contains a substring, say `"abc"`? Show an example using pandas. How would you make that check case-insensitive?
15. How do you extract a substring or match a regex pattern from each string in a Series? For example, extracting the first 3 letters of each string, or pulling out a regex capture group.
16. Suppose you have a column with full names in the format `"LastName, FirstName"`. How could you split this into two columns: one for LastName and one for FirstName?
17. How can you remove leading and trailing whitespace from all strings in a Series?
18. If you wanted to replace all occurrences of a substring in a Series (for example, replace `"Inc."` with `"Incorporated"` in a company name column), what pandas operation would you use?
19. How does pandas handle operations on strings that are actually missing (NaN)? For example, what happens if you do `df['col'].str.upper()` on a column that has some NaN values?
20. In terms of performance, why is it better to use pandas’ vectorized string operations (like `str.contains` or `str.replace`) instead of iterating over Series with Python string methods?

## Performance Optimization and Memory Profiling

1. How can you check the memory usage of a pandas DataFrame? What methods or attributes would you use to get memory usage information? (For example, how do you include the memory of object-type elements?)
2. What does the `memory_usage(deep=True)` option do when called on a DataFrame or Series? Why might the memory reported with `deep=True` differ from the memory reported with the default?
3. You have a DataFrame with a column of Python objects (e.g., strings). What are some ways to reduce the memory usage of that column using pandas or NumPy?
4. How might you reduce the memory footprint of a DataFrame with mostly numeric (int/float) columns? (Hint: think about the specific `dtype` of those columns.)
5. Give an example of how to *downcast* a numeric column to a smaller dtype (for instance, from int64 to int8, or float64 to float32) and explain when it’s safe or appropriate to do so.
6. Why is it generally not recommended to use Python `for` loops to iterate over pandas DataFrame rows? What should you do instead for better performance in pandas?
7. When you absolutely need to loop through rows of a DataFrame in Python, which is typically faster: using `iterrows()` or using `itertuples()`? Why?
8. If you have to apply a complex operation that cannot be vectorized, what alternatives can you consider to improve performance (e.g., using NumPy functions, Cython, Numba, multiprocessing, etc.)?
9. How can using the categorical data type help with performance in pandas? Provide a scenario where converting to `category` saves memory or speeds up operations.
10. You notice that a certain DataFrame operation is very slow. What tools or techniques could you use to profile or time your code to identify the bottleneck (for example, in a Jupyter environment)?
11. Discuss how pandas handles multi-core processing. Do pandas operations automatically use multiple CPU cores? If not, how can you leverage multiple cores for pandas-like operations?
12. What is Dask, and how does its DataFrame API relate to pandas? When might you choose to use a Dask DataFrame instead of a pandas DataFrame for a task?
13. How can you read a very large CSV file with pandas without running out of memory? Describe at least two techniques (e.g., using `chunksize`, using `usecols` or `dtype`, etc.)
14. When using `pd.read_csv`, how can specifying the `dtype` of columns or using the `usecols` parameter improve performance or memory usage?
15. What is *chunking* in the context of reading large data files, and how do you implement it with pandas? Provide a short example using `pd.read_csv` or `pd.read_sql` with chunking.
16. If you only need to sample or preview a large dataset, what options does pandas provide to read just a portion of a file (without loading the entire file)?
17. Explain how the `pd.eval()` or `DataFrame.eval()` function can sometimes improve performance for certain operations in pandas. What does it do under the hood?
18. How can writing to certain formats (like Parquet or Feather) be better for performance compared to CSV? What role does Apache Arrow play in this context?
19. What is the trade-off when using the `inplace=True` parameter in pandas methods from a performance perspective? (Does using inplace avoid copies and actually save time/memory in modern pandas?)
20. When merging or joining large DataFrames, what are some considerations or practices to ensure it is done efficiently (for example, pre-sorting the data, indexing the join columns, etc.)?
21. If you have a very large dataset that doesn’t fit into memory, besides Dask what other tools or strategies could you use to analyze it? (Think of using databases/SQL, PySpark, or breaking the problem into smaller parts.)
22. How does pandas compare to NumPy in terms of performance for numerical computations? When might you prefer to use raw NumPy arrays or functions instead of pandas for intensive calculations?
23. Why might a pandas operation be much slower on a column of type `object` compared to a column of type `int` or `float`? Provide an example of an operation that would illustrate this difference.
24. Describe a scenario where using a Python list comprehension or generator could be more efficient than a pandas method like `.apply()`, and a scenario where the pandas vectorized method is definitely more efficient.
25. What are some ways to optimize a groupby operation that is slow? (Think about the number of groups, using vectorized aggregations when possible, or reducing the data before grouping.)
26. In pandas 2.x, there is an option to use PyArrow-backed data types for certain columns (like strings). How does using a PyArrow extension type potentially improve performance or memory usage for those columns?
27. What does the term *vectorization* mean in the context of pandas (and NumPy), and why is vectorized code usually faster than equivalent Python loops?
28. If you need to compute a new column based on a complex formula involving several existing columns, what approach in pandas is likely to be fastest (for example, using vectorized arithmetic vs. using `DataFrame.apply`)?
29. Explain how making unnecessary copies of data can impact performance. For example, what is the cost of using `df.append()` repeatedly, and what should you do instead to concatenate many pieces of data?
30. Pandas often issues a SettingWithCopyWarning to caution about chained indexing. Aside from the correctness issues, how can working with a *view* vs. a *copy* of data affect performance or memory usage?

## Custom Functions and `.apply`

1. When using `DataFrame.apply`, what is the difference between `axis=0` and `axis=1`? Give an example of each usage.
2. How does `DataFrame.apply` differ from `DataFrame.applymap` in terms of what each one operates on?
3. If you use `df.apply(func, axis=1)` on a DataFrame, what will `func` receive as its input for each call? What about if you use `axis=0`?
4. When a function passed to `DataFrame.apply` returns a pandas Series, what does pandas do with those Series objects to form the result?
5. What does the `result_type` parameter of `DataFrame.apply` do? Explain the effect of `result_type='expand'` and `result_type='broadcast'` on the result.
6. How can you use DataFrame.apply to produce multiple output columns from a single input row? (For instance, splitting one column’s string into two new columns via an apply.)
7. If you have a function that takes two arguments and you want to apply it to two columns of a DataFrame to produce a single Series result, how would you do that? *(Hint: use a lambda inside apply with `axis=1` to pass `row['col1']` and `row['col2']`.)*
8. What are some alternatives to using `apply(axis=1)` for operations that *can* be vectorized? Give an example of a task that is better done with a vectorized approach than with apply.
9. Explain how you would map the values of a Series to new values (e.g., replacing values using a dictionary or a custom function) using pandas.
10. What is the difference between `Series.map` and `Series.apply`? When might you use one over the other?
11. How could you use `apply` to invoke a NumPy function that isn’t element-wise by default? (For example, applying `np.linalg.norm` to each *row* considered as a vector.)
12. What happens if your function passed to apply raises an exception for one of the groups or rows? Does apply propagate errors or skip them?
13. If you need to pass additional arguments to a function used in apply (say your function takes another parameter besides the data), how can you do that with `apply` or `map`?
14. How would you use `applymap` to, say, round all numeric values in a DataFrame to 2 decimal places? What are potential downsides of using `applymap` on a large DataFrame?
15. You want to apply a function to each group in a GroupBy object (each group being processed as a sub-DataFrame). Which method would you use for that scenario?
16. Why is using `df.apply(..., axis=1)` to create a new column usually slower than constructing that column with vectorized operations? Can you roughly quantify how much slower it might be for large data?
17. Provide an example where using `df.itertuples()` might be more appropriate (or faster) than using `df.apply()`.
18. How can list comprehensions sometimes be used in place of `apply` for simple row-wise operations, and how do they compare in performance?
19. In what scenario might `df.apply` return a Series as the result vs. return a DataFrame? Give examples of functions that cause each outcome.
20. Is it possible to use `apply` on an expanding or rolling window object? If so, how does that differ from using apply on an entire DataFrame?

## Working with Multi-Indexes (Hierarchical Indexing) and Reshaping

1. What is a MultiIndex in pandas, and how does it differ from a regular Index?
2. How can you create a MultiIndex for the rows of a DataFrame? Describe one method (for example, using `set_index` with multiple columns, or using `pd.MultiIndex.from_arrays`).
3. How do you access a subset of a DataFrame using a MultiIndex? For instance, if you have a DataFrame indexed by Country and Year (two-level index), how would you select all data for `Country="USA"` for all years?
4. How would you select data for `Country="USA"` and `Year=2020` from a DataFrame with a MultiIndex on (Country, Year)? Provide an example using `.loc`.
5. What is `pd.IndexSlice` and how is it used with MultiIndex DataFrames? Provide an example of slicing a multi-level index using `IndexSlice`.
6. How can you get all entries of a MultiIndex DataFrame for which one level has a specific value? (For example, all data where `Year = 2020`, across all outer index levels.)
7. Explain how the `.xs` (cross-section) method works on a MultiIndex DataFrame. How would you use it to select all data for `Year = 2020` from a DataFrame indexed by (Country, Year)?
8. How can you add names to the levels of a MultiIndex? Why is naming index levels helpful?
9. How would you rearrange (swap) the levels of a MultiIndex on a DataFrame? Which method would you use to swap index levels?
10. If you have a MultiIndex on a DataFrame, how can you “flatten” it back to a single-level Index? (For example, turning the index levels into ordinary columns.)
11. What are the `stack()` and `unstack()` methods? Give an example of using `unstack` to pivot a level of the index into the columns of the DataFrame.
12. If you call `.unstack()` on a Series or DataFrame with a MultiIndex, what does the resulting DataFrame look like (in terms of its index and columns)?
13. What is the difference between `pivot` and `pivot_table` in pandas? When would you use `pivot_table` instead of `pivot`?
14. How do you pivot a DataFrame from *long* format to *wide* format? Describe the roles of the `index`, `columns`, and `values` arguments in the `pivot` (or `pivot_table`) operation.
15. What happens if the data you want to pivot has duplicate entries for the combination of index and column keys? Which function would you use in that case, and how does it handle duplicates?
16. After performing a groupby with multiple aggregation functions, you often get a MultiIndex on the columns. How can you flatten these column names into single-level names (for example, by concatenating the names or using `map` to join them)?
17. How can you join or merge a DataFrame with a MultiIndex (on rows) to another DataFrame? (For example, matching on one level of the MultiIndex – what parameters would you use in merge?)
18. Can you have a MultiIndex on the *columns* of a DataFrame? Give an example of how a columns MultiIndex might be created (hint: the result of a `pivot_table` or `groupby.agg` with multiple metrics).
19. How do you remove a level from a MultiIndex without losing the information? (Hint: you might use `reset_index(level=...)]` or `droplevel`.)
20. What does it mean for a MultiIndex to be “lexicographically sorted,” and why might that matter for certain operations or slicing?

## Interoperability with SQL, Excel, NumPy, and Others

1. How can you read data from a SQL database into a pandas DataFrame? Which function would you use and what information do you need to provide (describe two ways: one using a SQL query, one using a table name)?
2. How do you write a pandas DataFrame to a SQL database table? What libraries or connection objects are typically involved in this process?
3. When using `pandas.read_sql()` or `DataFrame.to_sql()`, what is the role of **SQLAlchemy**? Can you use these functions without SQLAlchemy?
4. How would you approach loading a very large table from SQL into pandas if it doesn’t fit in memory? *(Hint: consider reading in chunks or using query limits/filters to pull smaller pieces.)*
5. Describe how to read an Excel (.xlsx or .xls) file into pandas. What function do you use and how can you specify which sheet or sheets to read?
6. How can you read **multiple sheets** from the same Excel file into pandas in one go?
7. How do you export a DataFrame to an Excel file? How would you include multiple DataFrames in one Excel workbook (multiple sheets)?
8. What are some limitations or considerations when using pandas with Excel files (with regard to data types, file size, or Excel’s own limitations)?
9. How can you convert a pandas DataFrame to a NumPy array? Provide an example using either `.to_numpy()` or the older `.values` attribute.
10. If a DataFrame has mixed data types across its columns, what will `df.to_numpy()` return (in terms of dtype)? What are the potential pitfalls of converting a heterogeneous DataFrame to a NumPy array?
11. Does modifying a NumPy array obtained from `df.values` or `df.to_numpy()` also modify the original DataFrame? Under what conditions might this happen (or not happen)?
12. Give an example of using a NumPy universal function (ufunc) on a pandas Series or DataFrame. (For instance, applying `np.log` to a numeric column or `np.sqrt` to an entire DataFrame of positive numbers.)
13. What is Apache Parquet and how do you read from or write to a Parquet file using pandas? Why might you choose Parquet over CSV for large datasets?
14. Pandas can interface with Apache Arrow (which Parquet uses under the hood). What benefits does Arrow provide for pandas, and how can you enable pandas to use Arrow for certain operations or data types (like using the new Arrow string dtype)?
15. Can pandas directly read data from a JSON file or an HTML page? If so, name the relevant functions and a brief description of what they do (for example, `pd.read_json`, `pd.read_html` and how they work).

## Reading/Writing Large Datasets Efficiently

1. What are some ways to efficiently read a **large CSV** file in pandas if it doesn’t fit into memory all at once?
2. How can you use the `chunksize` parameter in `pd.read_csv`? Give an example of processing a file in smaller chunks rather than reading it all at once.
3. If you only need a few columns from a very large CSV, what option can you use with `read_csv` to save memory and time?
4. How do you read a compressed CSV (for example, a CSV file compressed as .gz or .zip) with pandas? Does pandas handle decompression automatically?
5. When writing a DataFrame to CSV, how can you reduce the file size? (Consider using compression or limiting precision/columns.)
6. Compare the speed and file size of writing a DataFrame to CSV versus writing to a **binary format** like Parquet or Feather.
7. What is an advantage of using a binary format such as **Parquet** or **Feather** for large datasets?
8. How would you save a DataFrame so that when you load it back, you retain its data types and it loads faster than reading a CSV? (Name at least two formats that serve this purpose.)
9. What is a potential drawback of using pickling (`df.to_pickle`) to store DataFrames?
10. How can you limit the number of rows read from a file (for example, if you only want to read the first 1000 rows of a CSV to peek at the data)?
11. What does the `skiprows` parameter do in file reading functions like `pd.read_csv`?
12. How can you combine pandas with an iterator or loop to process a dataset that is too large to hold in memory at once? (For example, reading a file chunk-by-chunk and aggregating results.)
13. Discuss the trade-offs between the default C engine and the Python engine in `pd.read_csv`. When might you need to use the Python engine even though it’s slower?
14. What is the purpose of the `memory_map` parameter in `pd.read_csv`, and can it help with performance?
15. If you have a DataFrame that is too large to fit into memory, what options do you have to still perform analysis with pandas? (Think about downsizing the data, using external tools, etc.)
16. How would you export a very large DataFrame to an Excel file, and what limitations should you be aware of (from both pandas and Excel’s side)?
17. Why might writing a DataFrame to an Excel file be significantly slower than writing to a CSV or a binary format?
18. If you needed to merge or join two very large datasets that together cannot fit in memory, how might you approach this problem using pandas or other tools?
19. How does using `iterator=True` in `pd.read_sql` or `pd.read_csv` differ from using `chunksize`? (What do these options allow you to do?)
20. What are some best practices for handling large datasets with pandas to avoid performance bottlenecks or memory errors?

## Debugging and Edge Cases

1. What is a **SettingWithCopyWarning** in pandas, and why does it occur? Give an example of an operation that would trigger this warning.
2. How can you safely modify a slice of a DataFrame (for example, a subset obtained via a condition) without encountering a SettingWithCopyWarning?
3. Why does using the Python `and`/`or` operators with pandas Series or DataFrames (for example, `if (df['A'] > 5) and (df['B'] > 5):`) result in an error? What operators or methods should you use instead for element-wise logical comparisons?
4. When filtering a DataFrame with a boolean Series from another DataFrame, you might get an "Unalignable boolean Series provided" error. What does this mean and how can you avoid it?
5. If a DataFrame’s index has duplicate labels, how do certain operations behave? For instance, what does `df.loc['label']` return if there are multiple rows with the index `'label'`?
6. What problems can arise from having a column of numeric data stored as dtype `object` (i.e., numbers stored as Python objects)? Give an example of an operation that would be slower or could behave unexpectedly with object-dtype numbers.
7. Why might two float values that appear the same not compare as equal in pandas (or Python in general)? What can you do to safely compare floating-point numbers for “closeness” rather than exact equality?
8. You attempt to pivot a DataFrame and get an error about duplicate entries for the index/columns combination. What does this mean, and how can you resolve it?
9. When reading a CSV, you find that a column with values like `"00123"` is read as the integer `123`, dropping leading zeros. How can you ensure that such data is read in as strings (preserving leading zeros)?
10. A merge operation resulted in a much larger DataFrame than either of the inputs. What is a likely cause for this “data explosion,” and how can you check or prevent it?
11. After concatenating two DataFrames, you notice the index has duplicate values which might cause problems. What parameter of `pd.concat` can help detect or prevent unintended duplicate index entries?
12. When working with time series data, what issues might you encounter around Daylight Savings Time (DST) transitions or time zone conversions? Can you give an example of a potential pitfall?
13. If you divide one integer Series by another (e.g., `df['A'] / df['B']`) you end up with floats even when they divide evenly. Why does this happen, and how could you get an integer result instead if you wanted one?
14. Why is it not recommended to use `df1 == df2` to compare if two DataFrames are identical? What method should you use instead to compare DataFrame equality (including NaNs)?
15. Explain an edge case where `groupby().transform()` might not behave as expected. (For example, if your transformation function returns a scalar or if you try to transform multiple columns using a function that isn’t aggregation or element-wise.)
16. What does the error `"The truth value of a DataFrame is ambiguous"` mean, and when might you encounter it?
17. How would you go about debugging a performance issue in your pandas code? Suppose a certain operation is much slower than expected – what steps or tools would you use to diagnose the issue?
18. If a DataFrame operation causes your program to run out of memory, what strategies could you use to identify which part of your data or code is using too much memory?
19. Have you encountered the warning or error: *"A value is trying to be set on a copy of a slice from a DataFrame"*? What are some ways to rewrite code to avoid this situation?
20. In pandas, why might using chained indexing like `df[df['col'] > 0]['other_col'] = 5` not work as intended? What is the recommended way to do such an operation instead?
